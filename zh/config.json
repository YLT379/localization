{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "显示所有",
  "basicSettings": "基本设置",
  "configSubtitle": "加载或保存预设并尝试修改模型参数",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试修改影响预测的参数。",
  "generalParameters/title": "常规参数",
  "samplingParameters/title": "采样参数",
  "basicTab": "基本",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开全部",
  "advancedTab/overridesTitle": "配置 Overrides",
  "advancedTab/noConfigsText": "您还没有未保存的更改 - 编辑上面的值以查看 overrides.",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "没有可配置的参数",
  "generationParameters/info": "尝试修改影响文本生成的基准参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "用于控制模型的初始化方式以及加载到内存中的方式。",
  "loadParameters/reload": "重新加载以应用更改",
  "discardChanges": "丢弃更改",
  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段为模型提供背景说明，例如一组规则、约束或一般要求。这个字段通常也被称为“系统提示”。",
  "llm.prediction.systemPrompt/subTitle": "对 AI 的指导",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档:\n默认值为 <{{dynamicValue}}>，这在随机性和确定性之间取得了平衡。在极端情况下，温度为0时，始终会选择最有可能的下一个标记，从而导致每次运行输出相同的结果",
  "llm.prediction.llama.topKSampling/title": "Top K 采样",
  "llm.prediction.llama.topKSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-k 采样是一种文本生成方法，它仅从模型预测的最有可能的 top k 个标记中选择下一个标记。\n\n它有助于减少生成低概率或非法的标记的风险，但同时也可能限制输出的多样性。\n\n更大的 top-k 值（例如 100）将考虑更多标记并导致更具多样性的文本，而较小的值（例如 10）则会更加关注最可能的标记并生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/info": "计算过程中使用的线程数量。增加线程数量并不总是导致性能提升。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制 AI 响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人的响应最大长度。启用此选项可以设置响应长度上限，禁用此选项则让聊天机器人决定何时停止生成响应。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度 (标记)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "约 {{maxWords}} 个词",
  "llm.prediction.llama.repeatPenalty/title": "重复惩罚",
  "llm.prediction.llama.repeatPenalty/info": "来自 llama.cpp 帮助文档:\n\n有助于防止模型生成重复或单调的文本。\n\n更大的值（例如 1.5）会更强地惩罚重复，而较小的值（例如 0.9）则会更加宽松。 • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "最小 P 采样",
  "llm.prediction.llama.minPSampling/info": "来自 llama.cpp 帮助文档:\n\n相对于最有可能的标记，一个标记被视为可行的最小概率。必须在 [0, 1] 中。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 采样",
  "llm.prediction.llama.topPSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-p 采样，也称为核采样，是另一种文本生成方法，它从具有累积概率至少为 p 的标记子集（所有标记的概率之和）中选择下一个标记。\n\n这种方法通过考虑标记的概率及其需要采样的数量，在多样性和质量之间取得了平衡。\n\n更大的 top-p 值（例如 0.95）将导致更具多样性的文本，而较小的值（例如 0.5）则会生成更加集中和保守的文本。必须在 (0, 1] 中。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应该阻止模型生成更多标记的字符串",
  "llm.prediction.stopStrings/info": "当遇到这些特定的字符串时，将阻止模型生成更多标记。",
  "llm.prediction.stopStrings/placeholder": "输入一个字符串并按下⏎键",
  "llm.prediction.contextOverflowPolicy/title": "对话溢出",
  "llm.prediction.contextOverflowPolicy/info": "决定模型的上下文（工作记忆）大小超过时应该做什么",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "在限制处停止",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "当模型的内存满了时，停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "截断中间部分",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "从对话的中间删除消息以腾出空间。模型仍会记住对话的开头部分",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滚动窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型将始终获取最近的一些消息，但可能会忘记对话的开头部分",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.mlx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.mlx.repeatPenalty/info": "更高的值会让模型避免重复自己",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档:\n\n保留的用于 top-k-过滤的高概率词汇表标记数量\n\n• 此过滤器默认情况下关闭。",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/info": "更高的值会让模型避免重复自己",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档:\n\n仅保留概率总和达到 TopP 或更高的最高概率标记。\n\n• 此过滤器默认情况下关闭。",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "无法解析 Jinja 模板：{{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手动",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系统前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系统后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "输入系统后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "用户前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "输入用户前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "用户后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "输入用户后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助手前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助手后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
  "llm.prediction.promptTemplate.stopStrings/label": "额外的停止字符串",
  "llm.prediction.promptTemplate.stopStrings/hint": "模板特定的停止字符串，这些字符串将除了用户指定的停止字符串之外还会被使用。",
  
  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大标记数量，从而影响它在处理过程中保留的上下文量。",
  "llm.load.seed/title": "种子",
  "llm.load.seed/info": "随机数生成器：设置种子以确保可重复的结果",

  "llm.load.llama.evalBatchSize/title": "评估批处理大小",
  "llm.load.llama.evalBatchSize/info": "设置评估过程中一次处理的示例数量，影响速度和内存使用量",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "llm.load.llama.ropeFrequencyBase/info": "[高级]调整旋转位置编码的基础频率，影响如何嵌入位置信息",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "llm.load.llama.ropeFrequencyScale/info": "[高级]修改旋转位置编码的频率缩放以控制位置编码粒度",
  "llm.load.llama.gpuOffload/title": "GPU卸载",
  "llm.load.llama.gpuOffload/info": "设置卸载到 GPU 的计算比例。设置为 off 禁用 GPU 卸载，或设置为 auto 让模型自行决定。",
  "llm.load.llama.flashAttention/title": "闪现注意力",
  "llm.load.llama.flashAttention/info": "加速注意力机制，实现更快更高效的处理",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但会占用更高的 RAM 使用量",
  "llm.load.llama.useFp16ForKVCache/title": "使用 FP16 用于 KV 缓存",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度 (FP16) 存储缓存来减少内存使用量",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/info": "将模型文件直接从磁盘加载到内存中",
  
  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/info": "指定模型一次可以处理的最大令牌数，影响它在处理过程中保留的上下文量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级]调整旋转位置编码的基础频率，影响如何嵌入位置信息",
  "embedding.load.llama.evalBatchSize/title": "评估批处理大小",
  "embedding.load.llama.evalBatchSize/info": "设置评估过程中一次处理的令牌数量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级]修改旋转位置编码的频率缩放以控制位置编码粒度",
  "embedding.load.llama.gpuOffload/title": "GPU卸载",
  "embedding.load.llama.gpuOffload/info": "设置卸载到 GPU 的计算比例。设置为 off 禁用 GPU 卸载，或设置为 auto 让模型自行决定。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但会占用更高的 RAM 使用量",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/info": "将模型文件直接从磁盘加载到内存中",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/info": "随机种子：设置随机数生成的种子，以确保结果可重复性" 
}
